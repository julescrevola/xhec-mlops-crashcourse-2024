{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines and Orchestration with Prefect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> âš ï¸  **Version**: This module has been created using Prefect 2.13.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Useful functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 - From previous lessons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib/config.py\n",
    "CATEGORICAL_COLS = [\"PULocationID\", \"DOLocationID\", \"passenger_count\"]\n",
    "\n",
    "DATA_DIRPATH = \"../../data\"\n",
    "MODELS_DIRPATH = \"../../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: Le module spÃ©cifiÃ© est introuvable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: Le module spÃ©cifiÃ© est introuvable."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: Le module spÃ©cifiÃ© est introuvable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: Le module spÃ©cifiÃ© est introuvable."
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mloguru\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictVectorizer\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_target\u001b[39m(\n\u001b[0;32m     14\u001b[0m     df: pd\u001b[38;5;241m.\u001b[39mDataFrame, pickup_column: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_pickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, dropoff_column: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_dropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the trip duration in minutes based on pickup and dropoff time\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\__init__.py:83\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     86\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    130\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\__init__.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscovery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m all_estimators\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version, threadpool_info\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmurmurhash\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m murmurhash3_32\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     _is_arraylike_not_scalar,\n\u001b[0;32m     30\u001b[0m     as_float_array,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     indexable,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Do not deprecate parallel_backend and register_parallel_backend as they are\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# needed to tune `scikit-learn` behavior and have different effect if called\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# from the vendored version or or the site-package version. The other are\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# utilities that are independent of scikit-learn so they are not part of\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# scikit-learn public API.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jules\\anaconda3\\envs\\nyc-taxi\\lib\\site-packages\\sklearn\\utils\\murmurhash.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.utils.murmurhash\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# lib/preprocessing.py\n",
    "from typing import List, Tuple\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from loguru import logger\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "def compute_target(\n",
    "    df: pd.DataFrame, pickup_column: str = \"tpep_pickup_datetime\", dropoff_column: str = \"tpep_dropoff_datetime\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute the trip duration in minutes based on pickup and dropoff time\"\"\"\n",
    "    df[\"duration\"] = df[dropoff_column] - df[pickup_column]\n",
    "    df[\"duration\"] = df[\"duration\"].dt.total_seconds() / 60\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_outliers(df: pd.DataFrame, min_duration: int = 1, max_duration: int = 60) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows corresponding to negative/zero\n",
    "    and too high target' values from the dataset\n",
    "    \"\"\"\n",
    "    return df[df[\"duration\"].between(min_duration, max_duration)]\n",
    "\n",
    "\n",
    "def encode_categorical_cols(df: pd.DataFrame, categorical_cols: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Encode categorical columns as strings\"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = CATEGORICAL_COLS\n",
    "    df.loc[:, categorical_cols] = df[categorical_cols].fillna(-1).astype(\"int\")\n",
    "    df.loc[:, categorical_cols] = df[categorical_cols].astype(\"str\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_x_y(\n",
    "    df: pd.DataFrame,\n",
    "    categorical_cols: List[str] = None,\n",
    "    dv: DictVectorizer = None,\n",
    "    with_target: bool = True,\n",
    ") -> Tuple[scipy.sparse.csr_matrix, np.ndarray, DictVectorizer]:\n",
    "    \"\"\"Extract X and y from the dataframe\"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = CATEGORICAL_COLS\n",
    "    dicts = df[categorical_cols].to_dict(orient=\"records\")\n",
    "\n",
    "    y = None\n",
    "    if with_target:\n",
    "        if dv is None:\n",
    "            dv = DictVectorizer()\n",
    "            dv.fit(dicts)\n",
    "        y = df[\"duration\"].values\n",
    "\n",
    "    x = dv.transform(dicts)\n",
    "    return x, y, dv\n",
    "\n",
    "\n",
    "def process_data(filepath: str, dv=None, with_target: bool = True) -> scipy.sparse.csr_matrix:\n",
    "    \"\"\"\n",
    "    Load data from a parquet file\n",
    "    Compute target (duration column) and apply threshold filters (optional)\n",
    "    Turn features to sparce matrix\n",
    "    :return The sparce matrix, the target' values and the\n",
    "    dictvectorizer object if needed.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(filepath, engine=\"pyarrow\")\n",
    "    if with_target:\n",
    "        logger.debug(f\"{filepath} | Computing target...\")\n",
    "        df1 = compute_target(df)\n",
    "        logger.debug(f\"{filepath} | Filtering outliers...\")\n",
    "        df2 = filter_outliers(df1)\n",
    "        logger.debug(f\"{filepath} | Encoding categorical columns...\")\n",
    "        df3 = encode_categorical_cols(df2)\n",
    "        logger.debug(f\"{filepath} | Extracting X and y...\")\n",
    "        return extract_x_y(df3, dv=dv)\n",
    "    else:\n",
    "        logger.debug(f\"{filepath} | Encoding categorical columns...\")\n",
    "        df1 = encode_categorical_cols(df)\n",
    "        logger.debug(f\"{filepath} | Extracting X and y...\")\n",
    "        return extract_x_y(df1, dv=dv, with_target=with_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-2 Helpers for this session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also have other helpers to show you prefect's features in the `helpers.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib/helpers.py\n",
    "from typing import Any\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        loaded_obj = pickle.load(f)\n",
    "    return loaded_obj\n",
    "\n",
    "\n",
    "def save_pickle(path: str, obj: Any):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Create workflow functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create five functions to complete the ML process :\n",
    "- `train_model`\n",
    "- `predict`\n",
    "- `evaluate_model`\n",
    "- A workflow function to perform the whole training process `train_model_workflow`\n",
    "    - Process data\n",
    "    - Train model\n",
    "    - Evaluate model\n",
    "- A workflow function to perform the whole prediction process `batch_predict_workflow`\n",
    "    - Process data without target column\n",
    "    - Predict\n",
    "\n",
    "\n",
    "For the last two functions, you can start without saving / loading artifacts add these steps after.\n",
    "Please think about what artifacts you'll need to save and load to pass from training to predict workflows.\n",
    "\n",
    "Start by coding these functions here in the notebook\n",
    "\n",
    "Then, test your code with the downloaded data (e.g. January to train and February to predict).\n",
    "\n",
    "Finally, copy your code in the `lib` folder in the `modeling.py` and `workflows.py` files and test your workflows again using such a command:\n",
    "\n",
    "```bash\n",
    "python lib/workflows.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-24 01:58:20.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model_workflow\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mProcessing training data...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:23.121\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m71\u001b[0m - \u001b[34m\u001b[1m../../data\\yellow_tripdata_2021-01.parquet | Computing target...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:23.159\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m73\u001b[0m - \u001b[34m\u001b[1m../../data\\yellow_tripdata_2021-01.parquet | Filtering outliers...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:23.256\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m75\u001b[0m - \u001b[34m\u001b[1m../../data\\yellow_tripdata_2021-01.parquet | Encoding categorical columns...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:24.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m77\u001b[0m - \u001b[34m\u001b[1m../../data\\yellow_tripdata_2021-01.parquet | Extracting X and y...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:31.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model_workflow\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mProcessing test data...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:31.604\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m71\u001b[0m - \u001b[34m\u001b[1m../../data\\yellow_tripdata_2021-02.parquet | Computing target...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:31.670\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m73\u001b[0m - \u001b[34m\u001b[1m../../data\\yellow_tripdata_2021-02.parquet | Filtering outliers...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:31.836\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m75\u001b[0m - \u001b[34m\u001b[1m../../data\\yellow_tripdata_2021-02.parquet | Encoding categorical columns...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:33.247\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m77\u001b[0m - \u001b[34m\u001b[1m../../data\\yellow_tripdata_2021-02.parquet | Extracting X and y...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:38.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model_workflow\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mTraining model...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:51.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model_workflow\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mMaking predictions and evaluating...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:51.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_model_workflow\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mSaving artifacts to ../../models...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:51.568\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m80\u001b[0m - \u001b[34m\u001b[1m../../data\\yellow_tripdata_2021-03.parquet | Encoding categorical columns...\u001b[0m\n",
      "\u001b[32m2024-10-24 01:58:53.249\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m82\u001b[0m - \u001b[34m\u001b[1m../../data\\yellow_tripdata_2021-03.parquet | Extracting X and y...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE for the training set is 7.371075699863973. The predicted durations for February 2021 are: [11.32712279 11.54092865 11.54092865 ... 13.78278948 20.56218322\n",
      " 23.34310491]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "def train_model(X: scipy.sparse.csr_matrix, y: np.ndarray) -> LinearRegression:\n",
    "    \"\"\"Trains a Linear Regression model taking as input the training set and target, returns the model fitted\"\"\"\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def predict(X: scipy.sparse.csr_matrix, model: LinearRegression) -> np.ndarray:\n",
    "    \"\"\"Takes as input the model previously fitted in train_model and the test set, returns the predictions\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Takes as input the predictions array and the true target, returns the root mean squared error between the two\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def train_model_workflow(\n",
    "    train_filepath: str,\n",
    "    test_filepath: str,\n",
    "    artifacts_filepath: Optional[str] = None,\n",
    ") -> dict:\n",
    "    \"\"\"Train a model and save it to a file\"\"\"\n",
    "    logger.info(\"Processing training data...\")\n",
    "    X_train, y_train, dv = process_data(filepath=train_filepath, with_target=True)\n",
    "    logger.info(\"Processing test data...\")\n",
    "    X_test, y_test, _ = process_data(filepath=test_filepath, with_target=True, dv=dv)\n",
    "    logger.info(\"Training model...\")\n",
    "    model = train_model(X_train, y_train)\n",
    "    logger.info(\"Making predictions and evaluating...\")\n",
    "    y_pred = predict(X_test, model)\n",
    "    rmse = evaluate_model(y_test, y_pred)\n",
    "\n",
    "    if artifacts_filepath is not None:\n",
    "        logger.info(f\"Saving artifacts to {artifacts_filepath}...\")\n",
    "        save_pickle(os.path.join(artifacts_filepath, \"dv.pkl\"), dv)\n",
    "        save_pickle(os.path.join(artifacts_filepath, \"model.pkl\"), model)\n",
    "\n",
    "    return {\"model\": model, \"dv\": dv, \"rmse\": rmse}\n",
    "\n",
    "\n",
    "def batch_predict_workflow(\n",
    "    input_filepath: str,\n",
    "    model: Optional[LinearRegression] = None,\n",
    "    dv: Optional[DictVectorizer] = None,\n",
    "    artifacts_filepath: Optional[str] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Make predictions on a new dataset\"\"\"\n",
    "    if dv is None:\n",
    "        dv = load_pickle(os.path.join(artifacts_filepath, \"dv.pkl\"))\n",
    "    if model is None:\n",
    "        model = load_pickle(os.path.join(artifacts_filepath, \"model.pkl\"))\n",
    "\n",
    "    X, _, _ = process_data(filepath=input_filepath, with_target=False, dv=dv)\n",
    "    y_pred = predict(X, model)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "trained_dict = train_model_workflow(\n",
    "    train_filepath=os.path.join(DATA_DIRPATH, \"yellow_tripdata_2021-01.parquet\"),\n",
    "    test_filepath=os.path.join(DATA_DIRPATH, \"yellow_tripdata_2021-02.parquet\"),\n",
    "    artifacts_filepath=MODELS_DIRPATH,\n",
    ")\n",
    "rmse = trained_dict[\"rmse\"]\n",
    "\n",
    "y_pred = batch_predict_workflow(\n",
    "    input_filepath=os.path.join(DATA_DIRPATH, \"yellow_tripdata_2021-03.parquet\"),\n",
    "    artifacts_filepath=MODELS_DIRPATH,\n",
    ")\n",
    "\n",
    "print(f\"The RMSE for the training set is {rmse}. The predicted durations for February 2021 are: {y_pred}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Setup and explore Prefect\n",
    "\n",
    "We are going to use [Prefect](https://docs.prefect.io/2.6/tutorials/first-steps/), an Open Source orchestration tool with a Python SDK.\n",
    "\n",
    "\n",
    "**WINDOWS USERS**:\n",
    "\n",
    "You might run into issues with Prefect on Windows. If you do, please follow [Prefects instructions](https://docs.prefect.io/2.13.7/getting-started/installation/#install-prefect) to install Prefect on your machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 Setup Prefect UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to implement tasks and flows with prefect, let's set up the UI in order to have a good visualization of our work.\n",
    "\n",
    "Steps :\n",
    "\n",
    "- Set an API URL for your local server to make sure that your workflow will be tracked by this specific instance :\n",
    "```\n",
    "prefect config set PREFECT_API_URL=http://0.0.0.0:4200/api\n",
    "```\n",
    "\n",
    "- Check you have SQLite installed ([Prefect backend database system](https://docs.prefect.io/2.13.7/getting-started/installation/#external-requirements)):\n",
    "```\n",
    "sqlite3 --version \n",
    "```\n",
    "\n",
    "- Start a local prefect server :\n",
    "```\n",
    "prefect server start --host 0.0.0.0\n",
    "```\n",
    "\n",
    "If you want to reset the database, run :\n",
    "```\n",
    "prefect server database reset\n",
    "```\n",
    "\n",
    "\n",
    "You can visit the UI at http://0.0.0.0:4200/dashboard\n",
    "\n",
    "![](images/starting_page.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Prefect tasks and flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prefect uses tasks and flows to build workflows](https://docs.prefect.io/2.13.7/tutorial/flows/).\n",
    "- Flows are like functions. They can take inputs, perform work, and return an output. In fact, you can turn any function into a Prefect flow by adding the @flow decorator\n",
    "- A task is any Python function decorated with a @task decorator called within a flow. You can think of a flow as a recipe for connecting a known sequence of tasks together. Tasks, and the dependencies between them, are displayed in the flow run graph, enabling you to break down a complex flow into something you can observe, understand and control at a more granular level.\n",
    "    - All tasks must be called from within a flow. Tasks may not call other tasks directly.\n",
    "    - Not all functions in a flow need be tasks. Use them only when their features are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PrefectImportError' from 'prefect.exceptions' (c:\\Users\\jules\\anaconda3\\Lib\\site-packages\\prefect\\exceptions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flow, task\n\u001b[0;32m      6\u001b[0m \u001b[38;5;129m@task\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_url\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m, params: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m     response \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\jules\\anaconda3\\Lib\\site-packages\\prefect\\__init__.py:117\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m import_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, package\u001b[38;5;241m=\u001b[39mpackage)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     module \u001b[38;5;241m=\u001b[39m import_module(module_name, package\u001b[38;5;241m=\u001b[39mpackage)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr_name)\n",
      "File \u001b[1;32mc:\\Users\\jules\\anaconda3\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Users\\jules\\anaconda3\\Lib\\site-packages\\prefect\\main.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import user-facing API\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deploy\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m State\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_run_logger\n",
      "File \u001b[1;32mc:\\Users\\jules\\anaconda3\\Lib\\site-packages\\prefect\\deployments\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msteps\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     find_prefect_directory,\n\u001b[0;32m      5\u001b[0m     initialize_project,\n\u001b[0;32m      6\u001b[0m     register_flow,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     run_deployment,\n\u001b[0;32m     11\u001b[0m     load_flow_from_flow_run,\n\u001b[0;32m     12\u001b[0m     load_deployments_from_yaml,\n\u001b[0;32m     13\u001b[0m     Deployment,\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jules\\anaconda3\\Lib\\site-packages\\prefect\\_internal\\compatibility\\migration.py:50\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PydanticCustomError\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrefectImportError\n\u001b[0;32m     52\u001b[0m MOVED_IN_V3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect.deployments.deployments:load_flow_from_flow_run\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect.flows:load_flow_from_flow_run\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect.deployments:load_flow_from_flow_run\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect.flows:load_flow_from_flow_run\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect.client:get_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect.client.orchestration:get_client\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     61\u001b[0m }\n\u001b[0;32m     63\u001b[0m upgrade_guide_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRefer to the upgrade guide for more information: https://docs.prefect.io/latest/resources/upgrade-agents-to-workers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'PrefectImportError' from 'prefect.exceptions' (c:\\Users\\jules\\anaconda3\\Lib\\site-packages\\prefect\\exceptions.py)"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "import httpx\n",
    "from prefect import flow, task\n",
    "\n",
    "\n",
    "@task\n",
    "def get_url(url: str, params: dict = None):\n",
    "    response = httpx.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "@flow(retries=3, retry_delay_seconds=5, log_prints=True)\n",
    "def get_repo_info(repo_name: str = \"PrefectHQ/prefect\"):\n",
    "    url = f\"https://api.github.com/repos/{repo_name}\"\n",
    "    repo_stats = get_url(url)\n",
    "    print(f\"{repo_name} repository statistics ðŸ¤“:\")\n",
    "    print(f\"Stars ðŸŒ  : {repo_stats['stargazers_count']}\")\n",
    "    print(f\"Forks ðŸ´ : {repo_stats['forks_count']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Create Prefect tasks and flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Create tasks and flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the decorators `@task` and `@flow` to create your first prefect flow : The Processing flow.\n",
    "\n",
    "Prefect will try to use by default different thread to run each task. If you want sequential steps, introduce this dependencies through the name of each task output.\n",
    "\n",
    "\n",
    "Steps:\n",
    "- Create a task for each function you created in the previous section. You can start by doing these in the notebook.\n",
    "- Test your code by calling the flows run with downloaded data (this can be done in the notebook too).\n",
    "- Update your files in the `lib` folder. You should now have completed all files except `deployment.py`.\n",
    "\n",
    "\n",
    "You can see registered flows in the UI :\n",
    "![Flows in Prefect UI](images/flows_ui.png)\n",
    "\n",
    "\n",
    "And visualize the run of a flow :\n",
    "![Flows in Prefect UI](images/flow_run.png)\n",
    "\n",
    "\n",
    "> [!Warning]\n",
    "> **Typing tasks and flows in prefect** :\n",
    "> Typing tasks in prefect is done as with any python code.\n",
    "> For flows, either use `validate_parameters=False` or define pydantic models for prefect to understand your NON DEFAULT typing (see extra section).\n",
    "> But if all tasks are typed, since flows are just set of tasks, it should be all good if we don't want to add a layer of complexity\n",
    "> `Default types` : str, int ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 Customize your flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can configure the properties and special behavior for your prefect tasks/flow in the decorator.\n",
    "For example, you can tell if you want to retry on a failure, set name or tags, etc... \\\n",
    "An example is given in the `helpers.py` file.\n",
    "```\n",
    "@task('name=failure_task', tags=['fails'], retries=3, retry_delay_seconds=60)\n",
    "def func():\n",
    "  ...\n",
    "\n",
    "```\n",
    "\n",
    "- Add names, tasks, and desired behavior to your tasks/flows\n",
    "- Test your code\n",
    "- Visualize in the local prefect UI\n",
    "\n",
    "If a task fails in the flow, it is possible to visualize which task fail and access the full log and traceback error\n",
    "by clicking on the tasks. \\\n",
    "We can also access run information inside de `state` object that can be returned by the flows using python code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Deploy your flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the workflows are defined, we can now schedule automatics runs for these pipelines.\n",
    "Let's assume that we have a process that tells us that our model need to be retrained weekly based on some performance analysis. We also receive data to predict each hour.\n",
    "\n",
    "Use prefect deployment object in order to :\n",
    "- Schedule complete ml process to run weekly\n",
    "- Schedule prediction pipeline to run each hour\n",
    "\n",
    "\n",
    "**Please note that you can test your code with the `to_deployment` here, however you'll have to move to scripts to test the deployment with `serve`.**\n",
    "\n",
    "You can deploy your flows by following [Prefect documentation here](https://docs.prefect.io/2.13.7/tutorial/deployments/#running-multiple-deployments-at-once).\n",
    "\n",
    "âš ï¸  Serving a model with prefect is a long-running command, meaning that you will need to run it in a separate terminal or in the background.\n",
    "Interupting the command will stop the deployment, but you'll be still be able to see it the UI.\n",
    "\n",
    "In the UI, you should be able see deployments:\n",
    "![Deployments in Prefect UI](images/deployments.png)\n",
    "\n",
    "\n",
    "And the scheduled runs for one deployment:\n",
    "![Scheduled runs in Prefect UI](images/scheduled_runs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PrefectImportError' from 'prefect.exceptions' (c:\\Users\\jules\\anaconda3\\Lib\\site-packages\\prefect\\exceptions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# hello_world.py\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flow, serve\n\u001b[0;32m      5\u001b[0m \u001b[38;5;129m@flow\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello world\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhello_world\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jules\\anaconda3\\Lib\\site-packages\\prefect\\__init__.py:117\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m import_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, package\u001b[38;5;241m=\u001b[39mpackage)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     module \u001b[38;5;241m=\u001b[39m import_module(module_name, package\u001b[38;5;241m=\u001b[39mpackage)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr_name)\n",
      "File \u001b[1;32mc:\\Users\\jules\\anaconda3\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Users\\jules\\anaconda3\\Lib\\site-packages\\prefect\\main.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import user-facing API\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deploy\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m State\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_run_logger\n",
      "File \u001b[1;32mc:\\Users\\jules\\anaconda3\\Lib\\site-packages\\prefect\\deployments\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msteps\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     find_prefect_directory,\n\u001b[0;32m      5\u001b[0m     initialize_project,\n\u001b[0;32m      6\u001b[0m     register_flow,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeployments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     run_deployment,\n\u001b[0;32m     11\u001b[0m     load_flow_from_flow_run,\n\u001b[0;32m     12\u001b[0m     load_deployments_from_yaml,\n\u001b[0;32m     13\u001b[0m     Deployment,\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jules\\anaconda3\\Lib\\site-packages\\prefect\\_internal\\compatibility\\migration.py:50\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PydanticCustomError\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrefectImportError\n\u001b[0;32m     52\u001b[0m MOVED_IN_V3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect.deployments.deployments:load_flow_from_flow_run\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect.flows:load_flow_from_flow_run\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect.deployments:load_flow_from_flow_run\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect.flows:load_flow_from_flow_run\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect.client:get_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect.client.orchestration:get_client\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     61\u001b[0m }\n\u001b[0;32m     63\u001b[0m upgrade_guide_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRefer to the upgrade guide for more information: https://docs.prefect.io/latest/resources/upgrade-agents-to-workers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'PrefectImportError' from 'prefect.exceptions' (c:\\Users\\jules\\anaconda3\\Lib\\site-packages\\prefect\\exceptions.py)"
     ]
    }
   ],
   "source": [
    "# hello_world.py\n",
    "from prefect import flow, serve\n",
    "\n",
    "\n",
    "@flow(name=\"Hello world\")\n",
    "def hello_world(name: str = \"world\"):\n",
    "    print(f\"Hello {name}!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hello_world_deployment = hello_world.to_deployment(\n",
    "        name=\"Hello world Deployment\",\n",
    "        version=\"0.1.0\",\n",
    "        tags=[\"hello world\"],\n",
    "        interval=600,\n",
    "        parameters={\"name\": \"John Doe\"},\n",
    "    )\n",
    "    # Above: can be tested in notebook. Below: must be called from python script\n",
    "    serve(hello_world_deployment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Extra concepts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1 Prefect workers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 Prefect typing using Pydantic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-taxi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
